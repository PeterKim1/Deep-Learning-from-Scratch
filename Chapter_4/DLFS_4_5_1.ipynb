{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLFS_4_5_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeWHbjmsRnbx"
      },
      "source": [
        "## 4.5 학습 알고리즘 구현하기\r\n",
        "### 4.5.1 2층 신경망 클래스 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XmN0HoDkPx8"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nFG2sPGStu6"
      },
      "source": [
        "def softmax(x):\r\n",
        "    if x.ndim == 2:\r\n",
        "        x = x.T\r\n",
        "        x = x - np.max(x, axis=0)\r\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\r\n",
        "        return y.T \r\n",
        "\r\n",
        "    x = x - np.max(x) # 오버플로 대책\r\n",
        "    return np.exp(x) / np.sum(np.exp(x))\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "    return 1 / (1 + np.exp(-x))   \r\n",
        "\r\n",
        "# One Hot Encoding\r\n",
        "# def cross_entropy_error(y, t):\r\n",
        "#     if y.ndim == 1:\r\n",
        "#         t = t.reshape(1, t.size)\r\n",
        "#         y = y.reshape(1, y.size)\r\n",
        "\r\n",
        "#     batch_size = y.shape[0]\r\n",
        "#     return -np.sum(t * np.log(y + 1e-7)) / batch_size\r\n",
        "\r\n",
        "# Not One Hot Encoding\r\n",
        "def cross_entropy_error(y, t):\r\n",
        "    if y.ndim == 1:\r\n",
        "        t = t.reshape(1, t.size)\r\n",
        "        y = y.reshape(1, y.size)\r\n",
        "        \r\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\r\n",
        "    if t.size == y.size:\r\n",
        "        t = t.argmax(axis=1)\r\n",
        "             \r\n",
        "    batch_size = y.shape[0]\r\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\r\n",
        "\r\n",
        "def _numerical_gradient_no_batch(f, x):\r\n",
        "    h = 1e-4 # 0.0001\r\n",
        "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\r\n",
        "    \r\n",
        "    for idx in range(x.size):\r\n",
        "        tmp_val = x[idx]\r\n",
        "        \r\n",
        "        # f(x+h) 계산\r\n",
        "        x[idx] = float(tmp_val) + h\r\n",
        "        fxh1 = f(x)\r\n",
        "        \r\n",
        "        # f(x-h) 계산\r\n",
        "        x[idx] = tmp_val - h \r\n",
        "        fxh2 = f(x) \r\n",
        "        \r\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\r\n",
        "        x[idx] = tmp_val # 값 복원\r\n",
        "        \r\n",
        "    return grad\r\n",
        "\r\n",
        "def numerical_gradient(f, X):\r\n",
        "    if X.ndim == 1:\r\n",
        "        return _numerical_gradient_no_batch(f, X)\r\n",
        "    else:\r\n",
        "        grad = np.zeros_like(X)\r\n",
        "        \r\n",
        "        for idx, x in enumerate(X):\r\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\r\n",
        "        \r\n",
        "        return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fjJ2xv2Ox7g"
      },
      "source": [
        "class TwoLayerNet:\r\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\r\n",
        "        self.params = {}\r\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\r\n",
        "        self.params['b1'] = np.zeros(hidden_size)\r\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\r\n",
        "        self.params['b2'] = np.zeros(output_size)\r\n",
        "\r\n",
        "    def predict(self, x):\r\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\r\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\r\n",
        "\r\n",
        "        a1 = np.dot(x, W1) + b1\r\n",
        "        z1 = sigmoid(a1)\r\n",
        "        a2 = np.dot(z1, W2) + b2\r\n",
        "        y = softmax(a2)\r\n",
        "\r\n",
        "        return y\r\n",
        "\r\n",
        "    # x : 입력 데이터, t : 정답 레이블\r\n",
        "    def loss(self, x, t):\r\n",
        "        y = self.predict(x)\r\n",
        "\r\n",
        "        return cross_entropy_error(y, t)\r\n",
        "\r\n",
        "    def accuracy(self, x, t):\r\n",
        "        y = self.predict(x)\r\n",
        "        y = np.argmax(y, axis = 1)\r\n",
        "        t = np.argmax(t, axis = 1)\r\n",
        "\r\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\r\n",
        "\r\n",
        "        return accuracy\r\n",
        "\r\n",
        "    def numerical_gradient(self, x, t):\r\n",
        "        loss_W = lambda W: self.loss(x, t)\r\n",
        "\r\n",
        "        grads = {}\r\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\r\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\r\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\r\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\r\n",
        "\r\n",
        "        return grads\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNHdIIiOi2gt",
        "outputId": "b8909668-24c2-4984-f2ef-856cdbe91d71"
      },
      "source": [
        "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\r\n",
        "print(net.params['W1'].shape)\r\n",
        "print(net.params['b1'].shape)\r\n",
        "print(net.params['W2'].shape)\r\n",
        "print(net.params['b2'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 100)\n",
            "(100,)\n",
            "(100, 10)\n",
            "(10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-1pzZpgkSJm",
        "outputId": "69793796-07ce-4b3c-ae7e-cfb22a44e1a2"
      },
      "source": [
        "x = np.random.rand(100, 784) # 더미 데이터\r\n",
        "y = net.predict(x)\r\n",
        "print(y.shape) # 100개에 대해서 예측한 값"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2K7eHsLnlq4",
        "outputId": "f513e4b4-59f8-4935-eee8-a5ca73017ea7"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "x = np.random.rand(10, 784)\r\n",
        "t = np.random.rand(10, 10)\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "grads = net.numerical_gradient(x, t)\r\n",
        "\r\n",
        "print(\"걸린 시간: \", time.time() - start)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "걸린 시간:  60.03191614151001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLPb9A6-onfQ"
      },
      "source": [
        "책 대로 100개 기준으로 돌렸는데, 너무 안 나와서 10개만 돌려보니 60초가 걸렸다.\r\n",
        "\r\n",
        "아마 100개 기준으로 돌리면 10분 정도 걸릴듯\r\n"
      ]
    }
  ]
}